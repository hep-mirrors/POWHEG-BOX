Instructions on how to run WpWp2j splitting the various steps and
performing calculations in parallel when possible. Read also
Docs/Manyseeds.pdf.

Running is most conveniently done in a separate directory,
for instance testrun.
The directory must contain the powheg.input file and a pwgseeds.dat file. 

Before compiling make sure that:
- fastjet is installed and fastjet-config is in the path
- ifort and/or gfortran is in the path
- ifort/gfortran libraries are in the environment variable LD_LIBRARY_PATH

It is possible to compile both with ifort or gfortran, without removing the objects files.
To create pwhg_main do  
"make FC=ifort pwhg_main"
or 
"make FC=gfortran pwhg_main"

If you use LHAPDF, make sure that you have a version compatible with the compiler
you are using, and insert the appropriate path for LHAPDFCONFIG in the Makefile.


The gfortran version we tested was 4.4.3. For older versions, the
compiler may not recognize the -J option in F90/Makefile.gfortran.
One may need to replace it with the -M option.

Enter the testrun directory:

$ cd testrun

A powheg.input, and a pwgseeds.dat file
is present there. When executing

$../pwhg_main
 enter which seed

the program will require to enter an index in the pwgseeds.dat file, that
specifies the line number where the seed of the random number generator
to be used for the run is stored.
All results generated by the run will be stored in files named *-[index].*.
When running on parallel CPU's, make sure that each parallel run has a different
index.

-------------
-- STEP 1) --
-------------

Consists of a single run to generate the grid. 
The grid must be generated with the option
"fakevirt 1" in powheg.input, which means that
the virtual term is replaced by a fake one proportional to the Born term.  

One needs at least 1000000 events and 2 iterations

ncall1 1000000
itmx1 2

It takes roughly 20 hours of CPU.

Stop setting  
"ncall2 0"

When running
$../pwhg_main
 enter which seed
enter 1 (or any valid index).

-------------
-- STEP 2) --
-------------

Runs in parallel can be performed now. 
Need to comment out fakevirt from powheg.input.

The runs must be performed in the same directory where STEP 1
was performed.

The integration and upper bound for the generation of btilde
can be performed with 50-100 runs with 2500-5000 calls (ncall2);

ncall2 5000
itmx2 1

Folding numbers: 
foldcsi   5    ! number of folds on csi integration
foldy     5    ! number of folds on  y  integration
foldphi   10   ! number of folds on phi integration

Time is about 100 hours of cpu for each run with ncall2=5000.

Stop setting "nubound 0" 

For running, for example, 100 parallel processes do:
$../pwhg_main
 enter which seed
enter an index each run,  (from 1 to 100). The pwgseeds.dat must contain at least 100
lines, each with a different seed.


-------------
-- STEP 3) --
-------------
Can be run in parallel.
The number of processes cannot be larger than the one used
in the previous step.
The run must be performed in the same directory,
after all processes in STEP 2 are completed.

Set 
"nubound 100000" 
Takes 7 hours per process.

Stop setting "numevts 0"

The calling sequence is the same as before.


-------------
-- STEP 4) --
-------------
Set numevts to the number of events you want per process.

Run in parallel. 
The number of processes cannot be larger than the one used
in the previous step.

At this point, files of the form pwgevents-[index].lhe are present in the
run directory. Count the events:
grep '/event' pwgevents-*.lhe | wc

Merge them into a single event file:
cat  pwgevents-*.lhe | grep -v '/LesHouchesEvents' > pwgevents.lhe

Now go back to the WpWp2j directory:

$ cd ../

Build analysis file (PYTHIA or HERWIG)

$ make FC=ifort main-PYTHIA-lhef

Run the analysis

$ cd testrun
../main-PYTHIA-lhef

The time required is now very modest compared to the previous steps.

The same analysis performed in the WpWp2j paper is performed. The results
are stored in pwgPOWHEG+PYTHIA-output.top. If you have installed topdrawer
you can view the file. The topdrawer file is a readeable ascii file. You can
take out of it the tables of the results for each distribution.


Settings used for the published paper:
Step 1:
ncall1 1000000
itmx1  2

Step 2:

132 processes,
ncall2 5000
itmx2  1


Step 3:

132 processes
nubound 50000

Step 4:

132 processes
numevts 1000

